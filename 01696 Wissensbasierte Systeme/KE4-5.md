# 5 Maschinelles Lernen
## 5.1 Definition des Lernens
Es gibt verschiedene Definition von *Lernen*. In allen Lernsystemen gibt es jedoch eine grundlegende Unterscheidung zwischen dem eigentlichen **Lernsystem** und dem **Performanzelement**. Es gibt vier Elemente in einem allgemeinen Lernsystem:

**1. Lernelement** nimmt Erfahrungen und Beobachtungen aus der Umgebung auf und erzeugt un modifiziert Wissen.
**2. Performanzelement** interagiert mit der Umgebung und wird vom Wissen gesteuert.
**3. Kritikelement** teil dem Lernelement mit wie erfolgreich es ist.
**4. Problemgenerator** erzeugt Aufgaben die zu neuen und informativen Erfahrungen führen sollen.

## 5.2 Klassifikation der Ansätze zum maschinellen Lernen
Systeme zum maschinellen Lernen werden entlang dreier Dimensionen klassifiziert:

1. zugrundeliegende **Lernstrategie** (Wie viele Informationen sind bereits vorgegeben?)
2. **Wissensrepräsentation** die genutzt wird
3. **Anwendungsbereich** des Lernsystems

Die klare Einordnung ist meistens nicht möglich sondern fließend.

### 5.2.1 Klassifikation gemäß der benutzten Lernstrategie
Mit dem Aufwand den der Lernende treiben muss, nimmt der Aufwand des Lehrenden ab.

**1. Direkte Eingabe neuen Wissens und Auswendiglernen:** keinerlei Inferenz oder Wissenstransformation; Lernen durch Speichern von Daten und Fakten (Datenbanksystem)
**2. Lernen durch Anweisungen:** aufbereitetes Wissen wird vorgegeben und vom Lernenden verarbeitet
**3. Lernen durch Deduktion:** aus vorhandenen Wissen wird deduktiv neues Wissen abgeleitet
**4. Lernen durch Analogie:** Neue Fakten und Fähigkeiten werden durch Anpassung von vorhandenen Wissen an neue Situationen gewonnen; ausgewähltes Wissen muss transformiert und an die neuen Anforderungen angepasst werden
**5. Lernen aus Beispielen:** Der Lernende muss eine allgemeine Konzeptbeschreibung erstellen die alle gegebenen Beispiele umfasst; Unterscheiden zwischen: Beispiele vom Lehrenden oder vom Lernenden oder aus der Umgebung; Unterschieden werden kann zwischen nur positiven Beispielen oder positiven und negativen Beispielen; Unterschieden werden kann zwischen allen Beispielen gleichzeitig oder inkrementell gegeben
**6. Lernen aus Beobachtungen und durch Entdeckungen:** ist eine generelle Ausprägung des induktiven Lernens und am anspruchvollsten; keinerlei Steuerung durch einen Lehrenden; Unterscheidung möglich zwischen passiven Beobachtungen und aktiven Experimenten

### 5.2.2 Klassifikation gemäß dem gelernten Typ von Wissen

**1. Parameter in algebraischen Ausdrücken:** Numerische Parameter oder Koeffizienten müssen an einen gegebenen algebr. Ausdruck angepasst werden um ein gewünschtes Verhalten zu erzielen.
**2. Entscheidungsbäume:** Generieren von Entscheidungsbäumen um zwischen Elementen einer Klasse zu unterscheiden.
**3. Formale Grammatiken:** Ausgehend von Beispielausdrücken einer Sprache sollen formale Grammatiken erlernt werden.
**4. Regeln:** Regeln können erzeugt, verallgemeinert, spezialisiert oder eine Komposition erstellt werden.
**5. Ausdrücke basierend auf formaler Logik**
**6. Begriffshierachien**

### 5.2.3 Klassifikation gemäß dem Anwendungsbereich
Einsatzfelder von Landwirtschaft über Spieleprogrammierung bis Spracherkennung. Diese Klassifikation wird im Kurs nicht vertieft.

## 5.3 Erlernen von Entscheidungsbäumen
Erlernen von Entscheidungsbäumen ist eine der einfachsten Formen induktiven Lernens und in der Praxis sehr erfolgreich.

### 5.3.1 Entscheidungsbäume
**Ziel:** Objekte mit Mengen von Attribut / Werte-Paaren werden einer Klasse zugeordnet.

- **Blätter** des Baums sind mit Wahrheitswert makiert (z.B.: Ja- / Nein-Entscheidung)
- **innere Knoten** sind mit Attributen markiert
- **Kanten** sind mit Attributwerten markiert

Objekte werden klassifiziert indem von der Wurzel ausgehend alle den jeweiligen Knoten zugehörigen Attribute untersucht. Der dem Blattknoten zugeordnete Wert entspricht der Klasse des Objekts.

### 5.3.2 Erzeugung von Regeln aus Entscheidungsbäumen
Jeder Pfad von der Wurzel zu einem Blattknoten entspricht einer logischen Formel in der Form einer **if-the**-Regel.

### 5.3.3 Generieren von Entscheidungsbäumen
Ein **Lernverfahren** generiert aus einer Menge von Beispielen (*Trainingsmenge*) einen Entscheidungsbaum. Ein *Beispiel* ist ein Attribut / Wert-Paar mit der entsprechenden Klassifikation (positiv oder negativ).

**Trivialer Ansatz:** Jedes Beispiel ist ein Pfad von der Wurzel zu einem Knoten. Problem ist die mangelnde Generalisierung für andere Fälle. **Ziel des Lernens** ist dass der Baum nicht nur die Beispiele korrekt wiedergibt sondern auch möglichst kompakt ist.

> **Occam's Razor:**
> Bevorzuge die einfachste Hypothese, die konsistent mit allen Beobachtungen ist.

Im Allgemeinen ist ein kleiner Entscheidungsbaum, der konsistent mit allen Beispielen ist, eher korrekt als ein großer, komplexer Entscheidungsbaum. Es existiert eine erfolgreiche Heuristik um kleine Entscheidungsbäume zu generieren. Idee: erst das *wichtigste* Attribut testen.

Die **Wichtigkeit eines Attributs** ist ein relativer Begriff und stark abhängig von der aktuellen Beispielmenge die noch zu klassifizieren ist. Nach der Auswahl eines Attributs wird das Verfahren rekursiv für die Restmenge wiederholt.

Viel Fälle sind das rekursive Lernproblem zu unterscheiden:

**1. Menge der Beispiele ist leer:** In der Trainingsmenge war kein Beispiel mit der entsprechenden Attribut / Wert-Kombination. Hier könnte ein Default-Wert zurückgeben werden.
**2. Alle Beispiele haben die gleiche Klassifikation:** Klassifikation zurückgeben
**3. Attributmenge leer aber noch Beispiele vorhanden:** Beispiele der Trainingsmenge sind vielleicht falsch oder es fehlen Attribute
**4. Es sind noch positive und negative Beispiele und eine nichtleere Attributmenge vorhanden:** Das nächste Attribut wird anhand der *Wichtigkeit* ausgewählt und der Algorithmus rekursive weiter ausgeführt.

### 5.3.4 Bewertung des Lernerfolgs und Anwendungen
Man kann den Lernerfolg verbessern, indem mehr Beispiele in die Trainingsmenge aufgenommen werden, insbesondere die, die vorher als falsch klassifiziert wurden.

Die Bewertung des Erfolgs kann durche eine separate Menge von Beispielen (**Testmenge**)  erfolgen. Je mehr Beispiele der Testmenge korrekt klassifiziert werden, desto höher ist der Lernerfolg zu bewerten.

### 5.3.5 Die induktiven Lernverfahren ID3 und C4.5
**TDIDT (Top-Down Induction of Decision Trees):** schrittweises Aufbauen des Entscheidungsbaums von der Wurzel an. Kern ist die Attributauswahl mit dem Ziel den Baum möglichst klein zu halten. Ein **ideales Attribut** wäre eins, das die verbleibenden Beispiele genau in posifive und negative Beispielmengen aufteilt.

Der Algorithmus **ID3** quantifiziert die Merkmale unter Hinzunahme des *Informationsgehalts* der Attribute. Der Informationsgehalt wird in der Einheit Bit angegeben.

> 1 Bit ist der Informationsgehalt einer Ja/Nein-Antwort zu der vorher keinerlei Informationen vorlagen, also die Wahrscheinlichkeit für Ja und Nein jeweils 0,5 war.

**Bedingte mittlere Information** TODO

**Informationsgewinn (information gain)** TODO

Das Lernsystem ID3 wählt als nächstes das Attribut aus, bei dem der Informationsgewinn *gain(a)* maximal ist. Das *wichtigste* Attribut ist ein relativer Begriff und immer abhängig von der verbleibenden Beispielmenge.

Der *(absolute) Informationsgewinn* hat jedoch den **Nachteil**, dass er Attribute mit zahlreichen Werten bevorzugt. Das führt im Extremfall zu unsinnigen Ergebnissen. Beispiel: eindeutige ID für jeden Patienten führt zu *n* verschiedenen Werten für *n* Patienten. Die bedingte, mittlere Information ist 0 Bit, der *gain* also maximal, das Attribut in dem Fall jedoch nutzlos.

Das System **C4.5** verwendet keinen absoluten sondern einen *normierten Informationsgewinn*.

## 5.4 Lernen von Konzepten
Ziel ist das allgemeinere Lernen von Konzepten (Teilmengen von Objekten). Das Lernen eines Konzepts wird hier als Suchvorgang im Raum aller möglichen Hypothesen aufgefasst.

Ausgangspunkt ist wie beim Lernen von Entscheidungsbäumen eine Menge von Trainingsbeispielen mit Attributen und einer "Entscheidung" (positives oder negatives Beispiel).

Um eine Hypothese ausdrücken zu können braucht es eine Sprache.

### 5.4.2 Allgeimeine Problemstellung
Ein Konzept teil eine Obermenge in zwei disjunkte Klassen auf: Elemente die das Konzept erfüllen und diejenigen die das nicht tun.

> **Definition:** Ein Konzept ist eine einstellige Funktion "c: M -> { 0, 1 }" über eine Grundmenge M. Die Elemente von M heißen Beispiele.

Die Menge aller positiven Beispiele für e wird auch **Extension von c** genannt.

> **Definition:** Vollständigkeit und Konsistenz von Konzepten. TODO

**Bewertungsfunktion**

1. Die **Güte der Klassifkation** lässt sich als Prozentsatz der richtig klassifiierten Elemente der gesamten Grundmenge ausdrücken.
2. **Kosten der Fehlklassifikation** ist die Sume der Kosten aller Fehlklassifikationen über der gesamten Grundmenge.

### 5.4.3 Repräsentation von Beispielen und Konzepten
Sowohl di Beispiele als auch die gelernten Konzeote müssen repräsentiert werdenn Es müssen zwei Sprachen vorhanden sein.

1. Beobachtungen die als Eingabe dienen sind in einer **Beispielsprache** geschrieben.
2. Die gelernten Generalisierungen werden in einer **Konzeptsprache** vom Lernprogramm ausgegeben. Auch *Generalisierungssprache* oder *Hypothesensprache*.

>**Definition: Ein Konzeptlernproblem** besthet aus
>1. einer Beispielsprache L_E
>2. einer Konzeptsrpache L_C
>3. einem zu erlernendes Zielkonzept c \in L_C
>4. einer Menge P von positiven Beispielen
>5. einer Menge N von negativen Beispielen

Das **Ziel des Konzeptlernproblems** ist: Bestimme ein Konzept h aus der Konzeptsprache, so dass h(e) = c(e) für alle Beispiele e \in L_E ist.

P und N dürfen idealerweise keine Fehler enthalten und L_C muss mächtig genug sein, um das gesuchte Konzept ausdrücken zu können.

### 5.4.4 Lernen von Konzepten als Suchproblem
Lernen von Konzepten kann als Suchproblen aufgefasst werden. Der Suchraum ist die Menge aller Hypothesen die gebildet werden können.

>**Definition ("Spezieller-als-Relation"):** TODO

Es gibt verschiedene Möglichkeiten diesen Suchraum zu durchsuchen:

1. **Kandidaten-Eliminations-Methode (theoretisch):** initialer Hypothesenraum H ist die Menge L_C mit *allen* ausdrückbaren Konzepten. Bei jedem neuen Beispiel wird aus H die Hypothese entfernt die nicht der vorgegebenen Klassifikation übereinstimmt.

2. **Suchrichtung speziell --> allgemein:** "Spezieller-als-Relation" wird auf L_C ausgenutzt. Als initiale Hypothese wird die speziellste Hypothese genommen. Mit jedem neuen positiven Beispiel das noch nicht von h abgedeckt wird, wird die Hypothese verallgemeinert.

3. **Suchrichtung allgemein --> speziell:** Allgemeinste Hypothese ist die intiale Hypothese. Bei jedem negativen Beispiel das von h fälschlicherweise abgedeckt wird, wird die Hypothese spezialisiert.

Die letzten beiden Methoden können unterschieden werden in *Breitensuche* oder *Tiefensuche mit Backtracking*. Hier wird ein Verfahren vorgestellt, bei dem beide Suchrichtungen gleichzeitig betrieben werden.

###5.4.5 Versionenräume
Zu jedem Zeitpunkt wird die Menge aller bis dahin noch möglichen Hypothesen repräsentiert. Vor der Verarbeitung des ersten Trainingsbeispiels besteht der aktuelle Versionenraum aus der Menge L_C aller Hypothesen. Bei jedem neuen Beispiel werden alle inkorrekten und unvollständigen Hypothesen eliminiert (**Kandidate-Eliminations-Methode**).

Die Begrenzungsmengen werden durch die **speziellste und allgemeinste Generalisierung** definiert.

###5.4.6 Das Versionenraum-Lernverfahren
Es handelt sich um ein inkrementelles Verfahren, bei dem S und G den Versionenraum repräsentieren. Für jedes neue Beispiel werden S und G geprüft und gegebenenfalls angepasst. Falls die Hypothese h nicht mit dem Beispiel e übereinstimmt können zwei Fälle auftreten:

1. e ist für h fälschlicherweise negativ
2. e ist für h fälschlicherweise positiv

Für jede Hypothese muss wie folgt vorgegangen werden:

1. e ist für s fälschlicherweise positiv: s ist zu allgemein und muss aus S entfernt werden
2. e ist für s fälschlicherweise negativ: s ist zu speziell und muss verallgemeinert werden
3. e ist für g fälschlicherweise positiv: g ist zu allgemein und muss spezialisiert werden
4. e ist für g fälschlicherweise negativ: g ist zu speziell und muss aus G entfernt werden

Wenn der Algorithmus **terminiert** liegt einer der drei folgenden Fälle vor:

1. S und / oder G ist leer: Es gibt keine konsistente Hypothese.
2. S und G sind identische einelementige Mengen: Die Hypothese h ist das einzige mit der Trainingsmenge konsistente Konzept.
3. S und G sind nicht leer und enthalten unterschiedliche Hypothesen: Alle im Versionenraum liegende Hypothesen sind konsistent bezüglich der Trainingsmenge.

### 5.4.8 Eigenschaften des Versionenraum-Lernverfahrens
Ein eindeutiges Zielkonzept ist erlernt, wenn S und G einelementig und identisch sind. Vorraussetzungen damit der Versionenraum gegen eine Hypothese konvergiert:

1. Die Trainingsmenge enthält keine Fehler.
2. Die Konzeptbeschreibung h liegt im Hypothesenraum L_C und beschreibt das Zielkonzept vollständig und korrekt.

### 5.4.9 Konzeptlernen mit Merkmalsbäumen
Merkmalsbäume strukturieren Attributwerte hierachisch, damit beim Zusammentreffen zweier unterschiedlicher Werte nicht gleich zum allgemeinsten Constraint verallgemeinert werden muss.

# 6 Data Mining und Wissensfindung in Daten
Zielsetzung beim **KDD** (knowledge Discovery in databases) ist viel allgemeiner als Entscheidungsbaumlernen (Klassifikation) und Versionenraum-Lernen (Beschreibung eines Konzepts):

> Finde relevante Zusammenhänge in (gopßen) Datenmengen.

## 6.1 KDD - Knowledge Discovery in Databases
*Data Mining* und *KDD* sind Oberbegriffe für die automatisierte Analyse von Daten. *DM* bezechnet meist den Vorgang der Analyse und *KDD* den gesamten Prozess inklusive Datenbereinigung, Interpretation, ...

Gründe für Bedeutung:

1. Schnellere Akquise von Informationen
2. Informationen als wertvoller "Rohstoff"
3. Großer Wert und Nutzen "guter" Daten in der Wirtschaft und Optimierung
4. Allgemeines Interesse der KI-Forscher, Reiz der Aufgabe

## 6.2 Der KDD-Prozess

1. Hintergrundwissen und Zielsetzung
2. Datenauswahl
3. Datenbereinigung
4. Datenreduktion- und projektion
5. Modellfunktionalität
6. Verfahrenswahl
7. Data Mining
8. Interpretation

Diese Schritte werden falls nötig mehrfach durchlaufen. Herzstück ist das Data Mining. Anschließend müssen die neuen Erkentnisse in das bisherige Wissenssystem eingearbeitet werden.
