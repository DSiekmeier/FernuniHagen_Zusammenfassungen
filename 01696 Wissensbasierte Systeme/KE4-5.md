# 5 Maschinelles Lernen
## 5.1 Definition des Lernens
Es gibt verschiedene Definition von *Lernen*. In allen Lernsystemen gibt es jedoch eine grundlegende Unterscheidung zwischen dem eigentlichen **Lernsystem** und dem **Performanzelement**. Es gibt vier Elemente in einem allgemeinen Lernsystem:

**1. Lernelement** nimmt Erfahrungen und Beobachtungen aus der Umgebung auf und erzeugt un modifiziert Wissen.
**2. Performanzelement** interagiert mit der Umgebung und wird vom Wissen gesteuert.
**3. Kritikelement** teil dem Lernelement mit wie erfolgreich es ist.
**4. Problemgenerator** erzeugt Aufgaben die zu neuen und informativen Erfahrungen führen sollen.

## 5.2 Klassifikation der Ansätze zum maschinellen Lernen
Systeme zum maschinellen Lernen werden entlang dreier Dimensionen klassifiziert:

1. zugrundeliegende **Lernstrategie** (Wie viele Informationen sind bereits vorgegeben?)
2. **Wissensrepräsentation** die genutzt wird
3. **Anwendungsbereich** des Lernsystems

Die klare Einordnung ist meistens nicht möglich sondern fließend.

### 5.2.1 Klassifikation gemäß der benutzten Lernstrategie
Mit dem Aufwand den der Lernende treiben muss, nimmt der Aufwand des Lehrenden ab.

**1. Direkte Eingabe neuen Wissens und Auswendiglernen:** keinerlei Inferenz oder Wissenstransformation; Lernen durch Speichern von Daten und Fakten (Datenbanksystem)
**2. Lernen durch Anweisungen:** aufbereitetes Wissen wird vorgegeben und vom Lernenden verarbeitet
**3. Lernen durch Deduktion:** aus vorhandenen Wissen wird deduktiv neues Wissen abgeleitet
**4. Lernen durch Analogie:** Neue Fakten und Fähigkeiten werden durch Anpassung von vorhandenen Wissen an neue Situationen gewonnen; ausgewähltes Wissen muss transformiert und an die neuen Anforderungen angepasst werden
**5. Lernen aus Beispielen:** Der Lernende muss eine allgemeine Konzeptbeschreibung erstellen die alle gegebenen Beispiele umfasst; Unterscheiden zwischen: Beispiele vom Lehrenden oder vom Lernenden oder aus der Umgebung; Unterschieden werden kann zwischen nur positiven Beispielen oder positiven und negativen Beispielen; Unterschieden werden kann zwischen allen Beispielen gleichzeitig oder inkrementell gegeben
**6. Lernen aus Beobachtungen und durch Entdeckungen:** ist eine generelle Ausprägung des induktiven Lernens und am anspruchvollsten; keinerlei Steuerung durch einen Lehrenden; Unterscheidung möglich zwischen passiven Beobachtungen und aktiven Experimenten

### 5.2.2 Klassifikation gemäß dem gelernten Typ von Wissen

**1. Parameter in algebraischen Ausdrücken:** Numerische Parameter oder Koeffizienten müssen an einen gegebenen algebr. Ausdruck angepasst werden um ein gewünschtes Verhalten zu erzielen.
**2. Entscheidungsbäume:** Generieren von Entscheidungsbäumen um zwischen Elementen einer Klasse zu unterscheiden.
**3. Formale Grammatiken:** Ausgehend von Beispielausdrücken einer Sprache sollen formale Grammatiken erlernt werden.
**4. Regeln:** Regeln können erzeugt, verallgemeinert, spezialisiert oder eine Komposition erstellt werden.
**5. Ausdrücke basierend auf formaler Logik**
**6. Begriffshierachien**

### 5.2.3 Klassifikation gemäß dem Anwendungsbereich
Einsatzfelder von Landwirtschaft über Spieleprogrammierung bis Spracherkennung. Diese Klassifikation wird im Kurs nicht vertieft.

## 5.3 Erlernen von Entscheidungsbäumen
Erlernen von Entscheidungsbäumen ist eine der einfachsten Formen induktiven Lernens und in der Praxis sehr erfolgreich.

### 5.3.1 Entscheidungsbäume
**Ziel:** Objekte mit Mengen von Attribut / Werte-Paaren werden einer Klasse zugeordnet.

- **Blätter** des Baums sind mit Wahrheitswert makiert (z.B.: Ja- / Nein-Entscheidung)
- **innere Knoten** sind mit Attributen markiert
- **Kanten** sind mit Attributwerten markiert

Objekte werden klassifiziert indem von der Wurzel ausgehend alle den jeweiligen Knoten zugehörigen Attribute untersucht. Der dem Blattknoten zugeordnete Wert entspricht der Klasse des Objekts.

### 5.3.2 Erzeugung von Regeln aus Entscheidungsbäumen
Jeder Pfad von der Wurzel zu einem Blattknoten entspricht einer logischen Formel in der Form einer **if-the**-Regel.

### 5.3.3 Generieren von Entscheidungsbäumen
Ein **Lernverfahren** generiert aus einer Menge von Beispielen (*Trainingsmenge*) einen Entscheidungsbaum. Ein *Beispiel* ist ein Attribut / Wert-Paar mit der entsprechenden Klassifikation (positiv oder negativ).

**Trivialer Ansatz:** Jedes Beispiel ist ein Pfad von der Wurzel zu einem Knoten. Problem ist die mangelnde Generalisierung für andere Fälle. **Ziel des Lernens** ist dass der Baum nicht nur die Beispiele korrekt wiedergibt sondern auch möglichst kompakt ist.

> **Occam's Razor:**
> Bevorzuge die einfachste Hypothese, die konsistent mit allen Beobachtungen ist.

Im Allgemeinen ist ein kleiner Entscheidungsbaum, der konsistent mit allen Beispielen ist, eher korrekt als ein großer, komplexer Entscheidungsbaum. Es existiert eine erfolgreiche Heuristik um kleine Entscheidungsbäume zu generieren. Idee: erst das *wichtigste* Attribut testen.

Die **Wichtigkeit eines Attributs** ist ein relativer Begriff und stark abhängig von der aktuellen Beispielmenge die noch zu klassifizieren ist. Nach der Auswahl eines Attributs wird das Verfahren rekursiv für die Restmenge wiederholt.

Viel Fälle sind das rekursive Lernproblem zu unterscheiden:

**1. Menge der Beispiele ist leer:** In der Trainingsmenge war kein Beispiel mit der entsprechenden Attribut / Wert-Kombination. Hier könnte ein Default-Wert zurückgeben werden.
**2. Alle Beispiele haben die gleiche Klassifikation:** Klassifikation zurückgeben
**3. Attributmenge leer aber noch Beispiele vorhanden:** Beispiele der Trainingsmenge sind vielleicht falsch oder es fehlen Attribute
**4. Es sind noch positive und negative Beispiele und eine nichtleere Attributmenge vorhanden:** Das nächste Attribut wird anhand der *Wichtigkeit* ausgewählt und der Algorithmus rekursive weiter ausgeführt.

### 5.3.4 Bewertung des Lernerfolgs und Anwendungen
Man kann den Lernerfolg verbessern, indem mehr Beispiele in die Trainingsmenge aufgenommen werden, insbesondere die, die vorher als falsch klassifiziert wurden.

Die Bewertung des Erfolgs kann durche eine separate Menge von Beispielen (**Testmenge**)  erfolgen. Je mehr Beispiele der Testmenge korrekt klassifiziert werden, desto höher ist der Lernerfolg zu bewerten.

### 5.3.5 Die induktiven Lernverfahren ID3 und C4.5
**TDIDT (Top-Down Induction of Decision Trees):** schrittweises Aufbauen des Entscheidungsbaums von der Wurzel an. Kern ist die Attributauswahl mit dem Ziel den Baum möglichst klein zu halten. Ein **ideales Attribut** wäre eins, das die verbleibenden Beispiele genau in posifive und negative Beispielmengen aufteilt.

Der Algorithmus **ID3** quantifiziert die Merkmale unter Hinzunahme des *Informationsgehalts* der Attribute. Der Informationsgehalt wird in der Einheit Bit angegeben.

> 1 Bit ist der Informationsgehalt einer Ja/Nein-Antwort zu der vorher keinerlei Informationen vorlagen, also die Wahrscheinlichkeit für Ja und Nein jeweils 0,5 war.

**Bedingte mittlere Information** TODO

**Informationsgewinn (information gain)** TODO

Das Lernsystem ID3 wählt als nächstes das Attribut aus, bei dem der Informationsgewinn *gain(a)* maximal ist. Das *wichtigste* Attribut ist ein relativer Begriff und immer abhängig von der verbleibenden Beispielmenge.

Der *(absolute) Informationsgewinn* hat jedoch den **Nachteil**, dass er Attribute mit zahlreichen Werten bevorzugt. Das führt im Extremfall zu unsinnigen Ergebnissen. Beispiel: eindeutige ID für jeden Patienten führt zu *n* verschiedenen Werten für *n* Patienten. Die bedingte, mittlere Information ist 0 Bit, der *gain* also maximal, das Attribut in dem Fall jedoch nutzlos.

Das System **C4.5** verwendet keinen absoluten sondern einen *normierten Informationsgewinn*.
